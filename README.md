# TorchNLP-Notes

This repo is a collection of my notes on all things related to Natural Language Processing and PyTorch implementation.
As only a beginner of PyTorch myself, the contents of the repo heavily rely on external resources. Conveniently, the current repo can be thought of curated notes based on my learning. 

## Resources (with acknowledgement first!!!)

This repo greatly benefited from the following public repos. I will keep adding as I learn more, but I greatly appreciate the books by Daniel Godoy and Dr. Masato Hagiwara. 

- [Deep Learning with PyTorch Step-by-Step by Daniel Godoy](https://github.com/dvgodoy/PyTorchStepByStep)
- [Real-World Natural Language Processing by Dr. Masato Hagiwara](https://github.com/mhagiwara/realworldnlp)
- [NLP with Transformers](https://github.com/nlp-with-transformers/notebooks)
- [spaCy project templates](https://github.com/explosion/projects)
- [PyTorch documentation](https://pytorch.org/docs/stable/index.html)



## Personal Learning Objectives

As a researcher in Applied Linguistics (focusing on Language Assessment and Corpus linguistics), my learning goals center on applications of neural network models into my research contexts. Specifically, these includes:

- [ ] Understanding how PyTorch works and how to train a basic NN model
- [ ] How to create datasets; load them (very important step!!)
- [ ] Fine-tuning Transformer models through PyTorch and Huggingface
- [ ] Reimplementing some cool NLP architectures
- [ ] Creating custom language components (for a applied language tasks) with AllenNLP and/or spaCy

More specific learning objectives are:

- [ ] Reimplementing [Transformer-SRL package](https://github.com/Riccorl/transformer-srl) with spaCy version 3
- [ ] Understanding spaCy [SpanCategorizer](https://spacy.io/api/spancategorizer/#_title) and train the pipeline with relatively small hand-annotated data
- [ ] Implementing domain adaptation of BERT and RoBERTa model to general academic English
